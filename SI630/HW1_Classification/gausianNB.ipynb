{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SI 630: Homework 1 - Classification  (Sangho Eum)\n",
    "\n",
    "## TASK 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- Given some choice of β to make predictions, we want to use the difference in our prediction Yˆ from the ground truth Y to update β. The gradient of the log likelihood tells us which direction (positive or negative) to make the update and how large the update should be. Write a function compute gradient to compute the gradient. Note that we can compute the whole gradient using \n",
    "∇ l l = X T ( Y − Yˆ )\n",
    "Note that Y is a binary vector with our ground truth (i.e., the training data labels) and Yˆ is\n",
    "the binary vector with the predictions. To get a sense of why this works, think about what\n",
    "gradient will equal if our prediction for item i, Yˆ is the same as the ground truth Y ; if we ii\n",
    "use this gradient to update our weight for βi, what effect will it have?\n",
    "\n",
    "-> Gradient is zero, so it does not make an impact. \n",
    "\n",
    "- Train your model on the training data learning rate=5e-5 (i.e., a very small number) and num steps = 300000. Make a plot of the log-likelihood every 10,000 steps. Did the model converge at some point (i.e., does the log likelihood remain stable)?\n",
    "\n",
    "-> Yes, it converges to some point. It will be getting more stable.\n",
    "\n",
    "- Change the learning rate to a much larger and much smaller value and repeat the train- ing procedure for each. Plot all three curves together and describe what you observe. You’re welcome (encouraged, even!) to try additional learning rates. If your model is very slow or if it converges quickly, you can also reduce the number of steps for this question.\n",
    "\n",
    "-> See my graph below. I created curves with learning rate 5e-3, 5e-5, 5e-7 respectively.\n",
    "\n",
    "- After training on the training data, use your logistic regression classifier to make predictions on the validation dataset and report your performance using the F1.\n",
    "\n",
    "-> 0.9600 with 5e-5 learning rate\n",
    "\n",
    "- Submit your best model’s predictions on the test data to Kaggle.\n",
    "\n",
    "-> I have submitted mine and score is 0.95979\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import library\n",
    "import csv\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "import logging, sys\n",
    "import math\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "dat = []\n",
    "dev = []\n",
    "stop_words = []\n",
    "with open('train.tsv') as tsvfile:\n",
    "  reader = csv.DictReader(tsvfile, dialect='excel-tab', quoting=csv.QUOTE_NONE)\n",
    "  for i in reader:\n",
    "      dat.append(i)\n",
    "\n",
    "with open('dev.tsv') as tsvfile:\n",
    "  reader2 = csv.DictReader(tsvfile, dialect='excel-tab', quoting=csv.QUOTE_NONE)\n",
    "  for i in reader2:\n",
    "      dev.append(i)\n",
    "\n",
    "with open('stopwords.txt', 'r') as file:\n",
    "    for line in file.readlines():\n",
    "        stop_words.append(line.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenize % better tokenize functions\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "def better_tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"(@[a-zA-Z0-9_]+)|(\\w+:\\/\\/\\S+)\", \" \", text)\n",
    "    text = re.sub('[^A-Za-z0-9 ]+', '', text)\n",
    "    features = text.split()\n",
    "    filtered_features = []\n",
    "    for feature in features:\n",
    "        if feature not in stop_words:\n",
    "            filtered_features.append(feature)\n",
    "    return filtered_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create matrix function\n",
    "def create_matrix(lst):\n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    data = []\n",
    "    vocabulary = {}\n",
    "    y_true = []\n",
    "\n",
    "    for i in lst:\n",
    "        y_true.append(int(i['class']))\n",
    "\n",
    "        text = better_tokenize(i['text'])\n",
    "        for k in text:\n",
    "            index = vocabulary.setdefault(k, len(vocabulary))\n",
    "            indices.append(index)\n",
    "            data.append(1)\n",
    "        indptr.append(len(indices))\n",
    "    a = csr_matrix((data, indices, indptr)).toarray()\n",
    "    return [a, np.array([y_true]).T, vocabulary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix with train.tsv data set \n",
    "train_set = create_matrix(dat)\n",
    "train_x = train_set[0]\n",
    "train_y = train_set[1]\n",
    "train_v = train_set[2]\n",
    "\n",
    "\n",
    "sample_x=[]\n",
    "sample_y = []\n",
    "\n",
    "for i in range(train_x.shape[0]):\n",
    "    sample_x.append(train_x[i])\n",
    "    \n",
    "sample_x=np.array(sample_x)\n",
    "\n",
    "for i in train_y:\n",
    "    sample_y.append(i)    \n",
    "\n",
    "sample_y = np.array(sample_y).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(sample_x, sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(d, vocabulary, label = True, add_intercept = False):\n",
    "    yp = []\n",
    "    yt = []\n",
    "    inst =[]\n",
    "    xt= []\n",
    "\n",
    "    for i in d:\n",
    "        inst.append(i['instance_id'])\n",
    "        if label:\n",
    "            yt.append(int(i['class']))\n",
    "\n",
    "\n",
    "        if add_intercept:\n",
    "            x = np.zeros((1, len(vocabulary)+1))\n",
    "            x[0, 0] = 1\n",
    "            text = better_tokenize(i['text'])\n",
    "            for k in text:\n",
    "                if k in vocabulary:\n",
    "                    index = vocabulary[k]\n",
    "                    x[0, index+1] += 1\n",
    "            xt.append(x)\n",
    "        else:\n",
    "            x = np.zeros((1, len(vocabulary)))\n",
    "            text = better_tokenize(i['text'])\n",
    "            for k in text:\n",
    "                if k in vocabulary:\n",
    "                    index = vocabulary[k]\n",
    "                    x[0, index] += 1\n",
    "            xt.append(x)\n",
    "\n",
    "    return {'x': xt, 'yt': yt, 'inst_id': inst}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_set = predict(dev, train_v, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 38010)\n"
     ]
    }
   ],
   "source": [
    "test_x = sample_set['x']\n",
    "test_yt = sample_set['yt']\n",
    "\n",
    "\n",
    "print(test_x[0].shape)\n",
    "sample_x1 =[]\n",
    "sample_yt1 = []\n",
    "\n",
    "for i in range(len(test_x)):     \n",
    "    sample_x1.append(test_x[i][0])\n",
    "sample_x1= np.array(sample_x1)\n",
    "\n",
    "for i in test_yt:\n",
    "    sample_yt1.append(i)\n",
    "\n",
    "sample_yt1 = np.array(sample_yt1).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 ..., 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "sample_yp1 = clf.predict(sample_x1)\n",
    "print(sample_yp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66324251898168829"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(sample_yp1, sample_yt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
